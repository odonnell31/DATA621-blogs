---
title: 'Blog 4: Measurements of a Set'
author: "Michael O'Donnell"
date: "October 20, 2020"
output:
  pdf_document: default
  html_document: default
---

In Blog Three I explored the ANOVA analysis.
In this Blog I will dive into Measurements of a Set including accuracy, precision, recall, and F1 score.

Now, start by loading a dataset
This dataset contains data for all NBA teams from 2014-2018
```{r load nba data}
nbaData <- read.csv("data/nba_data.csv")
colnames(nbaData)[1] <- "Team"
nbaData$WinningTeam <- nbaData$WinPercentage
nbaData$WinningTeam[nbaData$WinningTeam > .5] <- 1
nbaData$WinningTeam[nbaData$WinningTeam <= .5] <- 0
nbaData$PositivePtsDiff <- nbaData$Pts - nbaData$OppPts
nbaData$PositivePtsDiff[nbaData$PositivePtsDiff > 0] <- 1
nbaData$PositivePtsDiff[nbaData$PositivePtsDiff <= 0] <- 0

#head(nbaData, 1)
str(nbaData)
```

To look at measurements of a set, we will let our model assume that a positive points differential implies the team will be a winning team (>50% win percentage)
```{r confusion matrix}
# Loading the data
table(nbaData$PositivePtsDiff, nbaData$WinningTeam)
```


First, a function to look at the accuracy of our model.
Accuracy is the % of correct predictions out of all predictions.
```{r accuracy}
# write a function to calculate accuracy
accuracy <- function(df) {
  TruePositive <- nrow(df[df$PositivePtsDiff == 1 & df$WinningTeam == 1,])
  TrueNegative <- nrow(df[df$PositivePtsDiff == 0 & df$WinningTeam == 0,])
  FalsePositive <- nrow(df[df$PositivePtsDiff == 0 & df$WinningTeam == 1,])
  FalseNegative <- nrow(df[df$PositivePtsDiff == 1 & df$WinningTeam == 0,])
  
  acc <- round((TruePositive+TrueNegative)/
                 (TruePositive+TrueNegative+FalsePositive+FalseNegative), 3)
  
  return(acc)
}

accuracy(nbaData)
```

Second, a function to look at the precision of our model.
Precision is the % of True Positives out of all Positive predictions.
```{r precision}
# write a function to calculate precision
precision <- function(df) {
  TruePositive <- nrow(df[df$PositivePtsDiff == 1 & df$WinningTeam == 1,])
  TrueNegative <- nrow(df[df$PositivePtsDiff == 0 & df$WinningTeam == 0,])
  FalsePositive <- nrow(df[df$PositivePtsDiff == 0 & df$WinningTeam == 1,])
  FalseNegative <- nrow(df[df$PositivePtsDiff == 1 & df$WinningTeam == 0,])
  
  prec <- round((TruePositive)/(TruePositive+FalsePositive), 3)
  
  return(prec)
}

precision(nbaData)
```

Third, a function to look at the recall of our model.
Recall is the % of True Positives out of all True Positives and False Negative predictions.
```{r recall}
# write a function to calculate sensitivity
recall <- function(df) {
  TruePositive <- nrow(df[df$PositivePtsDiff == 1 & df$WinningTeam == 1,])
  TrueNegative <- nrow(df[df$PositivePtsDiff == 0 & df$WinningTeam == 0,])
  FalsePositive <- nrow(df[df$PositivePtsDiff == 0 & df$WinningTeam == 1,])
  FalseNegative <- nrow(df[df$PositivePtsDiff == 1 & df$WinningTeam == 0,])
  
  sens <- round((TruePositive)/(TruePositive+FalseNegative), 3)
  
  return(sens)
}

recall(nbaData)
```

Last, a function to look at the F1 Score of our model.
F1 Score is is blended metric of precision and recall
```{r f_one_score}
# write a function to calculate F1 Score
f_one_score <- function(df) {
  TruePositive <- nrow(df[df$PositivePtsDiff == 1 & df$WinningTeam == 1,])
  TrueNegative <- nrow(df[df$PositivePtsDiff == 0 & df$WinningTeam == 0,])
  FalsePositive <- nrow(df[df$PositivePtsDiff == 0 & df$WinningTeam == 1,])
  FalseNegative <- nrow(df[df$PositivePtsDiff == 1 & df$WinningTeam == 0,])
  
  f_one <- round((2*precision(df)*recall(df))/
                   (precision(df)+recall(df)), 3)
  
  return(f_one)
}

f_one_score(nbaData)
```

Overall, the accuracy, precision, recall, and F1 Score were all over 80%.